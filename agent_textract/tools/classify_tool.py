"""Document Classification Tool - Classify and extract data from documents"""

import sys
import os
import boto3
import logging
import json
import re
from typing import Dict, List, Any

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config
from strands import Agent, tool
from strands.models import BedrockModel

boto_session = boto3.Session(
    aws_access_key_id=config.AWS_ACCESS_KEY_ID,
    aws_secret_access_key=config.AWS_SECRET_ACCESS_KEY,
    aws_session_token=config.AWS_SESSION_TOKEN,
    region_name=config.AWS_REGION
)

bedrock_model = BedrockModel(
    boto_session=boto_session,
    model_id=config.CHATBOT_AGENT_MODEL,
    temperature=0.1,  
    max_tokens=5000,
)

@tool
def classify_document_type(extracted_text: str) -> str:
    """
    Ph√¢n lo·∫°i lo·∫°i gi·∫•y t·ªù d·ª±a tr√™n vƒÉn b·∫£n ƒë√£ tr√≠ch xu·∫•t
    
    Args:
        extracted_text: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ Textract
    
    Returns:
        K·∫øt qu·∫£ ph√¢n lo·∫°i v·ªõi category v√† document type
    """
    
    identity_keywords = [
        'driver', 'license', 'passport', 'id card', 'identity', 'citizen', 'national id',
        'b·∫±ng l√°i xe', 'h·ªô chi·∫øu', 'ch·ª©ng minh', 'cƒÉn c∆∞·ªõc', 'gi·∫•y t·ªù t√πy th√¢n'
    ]
    
    address_keywords = [
        'utility bill', 'bank statement', 'lease', 'rental', 'address', 'residence',
        'h√≥a ƒë∆°n ƒëi·ªán', 'h√≥a ƒë∆°n n∆∞·ªõc', 'sao k√™ ng√¢n h√†ng', 'h·ª£p ƒë·ªìng thu√™', 'ƒë·ªãa ch·ªâ'
    ]
    
    eligibility_keywords = [
        'certificate', 'diploma', 'degree', 'qualification', 'employment', 'income',
        'b·∫±ng c·∫•p', 'ch·ª©ng ch·ªâ', 'vƒÉn b·∫±ng', 'gi·∫•y ch·ª©ng nh·∫≠n', 'thu nh·∫≠p'
    ]
    
    text_lower = extracted_text.lower()
    
    identity_score = sum(1 for keyword in identity_keywords if keyword in text_lower)
    address_score = sum(1 for keyword in address_keywords if keyword in text_lower)
    eligibility_score = sum(1 for keyword in eligibility_keywords if keyword in text_lower)
    
    scores = {
        'Identity': identity_score,
        'Address': address_score, 
        'Eligibility': eligibility_score
    }
    
    category = max(scores, key=scores.get)
    confidence = max(scores.values())
    
    document_type = "Unknown"
    
    if category == "Identity":
        if any(word in text_lower for word in ['driver', 'license', 'b·∫±ng l√°i']):
            document_type = "Driver's License"
        elif any(word in text_lower for word in ['passport', 'h·ªô chi·∫øu']):
            document_type = "Passport"
        elif any(word in text_lower for word in ['id card', 'identity', 'cƒÉn c∆∞·ªõc', 'ch·ª©ng minh']):
            document_type = "ID Card"
    
    elif category == "Address":
        if any(word in text_lower for word in ['utility', 'electric', 'water', 'ƒëi·ªán', 'n∆∞·ªõc']):
            document_type = "Utility Bill"
        elif any(word in text_lower for word in ['bank', 'statement', 'ng√¢n h√†ng', 'sao k√™']):
            document_type = "Bank Statement"
        elif any(word in text_lower for word in ['lease', 'rental', 'thu√™']):
            document_type = "Lease Agreement"
    
    elif category == "Eligibility":
        if any(word in text_lower for word in ['certificate', 'ch·ª©ng ch·ªâ']):
            document_type = "Certificate"
        elif any(word in text_lower for word in ['diploma', 'degree', 'b·∫±ng']):
            document_type = "Diploma/Degree"
        elif any(word in text_lower for word in ['employment', 'income', 'thu nh·∫≠p']):
            document_type = "Employment Document"
    
    result = f"""üìã DOCUMENT CLASSIFICATION
    
üè∑Ô∏è Category: {category}
üìÑ Document Type: {document_type}
üéØ Confidence Score: {confidence}/10
üìä Scores: Identity({identity_score}) | Address({address_score}) | Eligibility({eligibility_score})

‚úÖ Classification completed!"""
    
    return result

@tool
def extract_key_fields(extracted_text: str, document_category: str) -> str:
    """
    Tr√≠ch xu·∫•t c√°c tr∆∞·ªùng d·ªØ li·ªáu quan tr·ªçng d·ª±a tr√™n lo·∫°i t√†i li·ªáu
    
    Args:
        extracted_text: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t
        document_category: Lo·∫°i t√†i li·ªáu (Identity/Address/Eligibility)
    
    Returns:
        C√°c tr∆∞·ªùng d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t
    """
    
    extracted_fields = {}
    text_lines = extracted_text.split('\n')
    
    patterns = {
        'name': r'(?:name|t√™n|h·ªç t√™n)[:\s]*([A-Za-z\s]+)',
        'date_of_birth': r'(?:dob|date of birth|ng√†y sinh|sinh)[:\s]*(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})',
        'id_number': r'(?:id|number|s·ªë)[:\s]*([A-Z0-9]+)',
        'address': r'(?:address|ƒë·ªãa ch·ªâ)[:\s]*([^,\n]+)',
        'phone': r'(?:phone|tel|ƒëi·ªán tho·∫°i)[:\s]*(\d{10,11})',
        'email': r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'
    }
    
    if document_category.lower() == "identity":
        for field, pattern in patterns.items():
            if field in ['name', 'date_of_birth', 'id_number', 'address']:
                matches = re.findall(pattern, extracted_text, re.IGNORECASE)
                if matches:
                    extracted_fields[field] = matches[0].strip()
    
    elif document_category.lower() == "address":
        for field, pattern in patterns.items():
            if field in ['name', 'address', 'phone']:
                matches = re.findall(pattern, extracted_text, re.IGNORECASE)
                if matches:
                    extracted_fields[field] = matches[0].strip()
    
    elif document_category.lower() == "eligibility":
        for field, pattern in patterns.items():
            if field in ['name', 'date_of_birth', 'id_number']:
                matches = re.findall(pattern, extracted_text, re.IGNORECASE)
                if matches:
                    extracted_fields[field] = matches[0].strip()
    
    # Format k·∫øt qu·∫£
    if extracted_fields:
        result = f"""üîç EXTRACTED KEY FIELDS ({document_category.upper()})
        
"""
        for field, value in extracted_fields.items():
            result += f"üìù {field.replace('_', ' ').title()}: {value}\n"
        
        result += "\n‚úÖ Field extraction completed!"
    else:
        result = f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y tr∆∞·ªùng d·ªØ li·ªáu n√†o cho category: {document_category}"
    
    return result

@tool
def detect_multiple_documents(extracted_text: str) -> str:
    """
    Ph√°t hi·ªán v√† x·ª≠ l√Ω tr∆∞·ªùng h·ª£p m·ªôt ·∫£nh c√≥ nhi·ªÅu lo·∫°i gi·∫•y t·ªù
    
    Args:
        extracted_text: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t
    
    Returns:
        Th√¥ng tin v·ªÅ c√°c t√†i li·ªáu ƒë∆∞·ª£c ph√°t hi·ªán
    """
    
    document_indicators = {
        "Driver's License": ['driver', 'license', 'class', 'endorsements', 'b·∫±ng l√°i'],
        "Passport": ['passport', 'nationality', 'place of birth', 'h·ªô chi·∫øu'],
        "ID Card": ['identity card', 'citizen', 'id number', 'cƒÉn c∆∞·ªõc', 'ch·ª©ng minh'],
        "Utility Bill": ['utility', 'electric', 'water', 'gas', 'h√≥a ƒë∆°n'],
        "Bank Statement": ['bank', 'statement', 'balance', 'account', 'ng√¢n h√†ng'],
        "Certificate": ['certificate', 'certify', 'awarded', 'ch·ª©ng ch·ªâ', 'ch·ª©ng nh·∫≠n']
    }
    
    detected_documents = []
    text_lower = extracted_text.lower()
    
    for doc_type, keywords in document_indicators.items():
        score = sum(1 for keyword in keywords if keyword in text_lower)
        if score >= 2:  # Threshold ƒë·ªÉ x√°c ƒë·ªãnh c√≥ t√†i li·ªáu
            detected_documents.append({
                'type': doc_type,
                'confidence': score
            })
    
    detected_documents.sort(key=lambda x: x['confidence'], reverse=True)
    
    if len(detected_documents) > 1:
        result = f"""üîç MULTIPLE DOCUMENTS DETECTED
        
üìä Found {len(detected_documents)} document types:

"""
        for i, doc in enumerate(detected_documents, 1):
            result += f"{i}. {doc['type']} (confidence: {doc['confidence']})\n"
        
        result += f"""
‚ö†Ô∏è RECOMMENDATION:
- X·ª≠ l√Ω t·ª´ng t√†i li·ªáu ri√™ng bi·ªát
- ∆Øu ti√™n t√†i li·ªáu c√≥ confidence cao nh·∫•t
- Ki·ªÉm tra l·∫°i k·∫øt qu·∫£ tr√≠ch xu·∫•t

‚úÖ Multi-document detection completed!"""
    
    elif len(detected_documents) == 1:
        result = f"""üìÑ SINGLE DOCUMENT DETECTED
        
üè∑Ô∏è Document Type: {detected_documents[0]['type']}
üéØ Confidence: {detected_documents[0]['confidence']}

‚úÖ Single document confirmed!"""
    
    else:
        result = "‚ö†Ô∏è Kh√¥ng th·ªÉ x√°c ƒë·ªãnh lo·∫°i t√†i li·ªáu r√µ r√†ng. C·∫ßn ki·ªÉm tra l·∫°i."
    
    return result

@tool
def handle_multiple_entities(extracted_text: str) -> str:
    """
    X·ª≠ l√Ω tr∆∞·ªùng h·ª£p m·ªôt t√†i li·ªáu c√≥ nhi·ªÅu th·ª±c th·ªÉ (ng∆∞·ªùi, ƒë·ªãa ch·ªâ, v.v.)
    
    Args:
        extracted_text: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t
    
    Returns:
        Th√¥ng tin v·ªÅ c√°c th·ª±c th·ªÉ ƒë∆∞·ª£c ph√°t hi·ªán
    """
    
    name_pattern = r'(?:name|t√™n|h·ªç t√™n)[:\s]*([A-Za-z\s]+)'
    address_pattern = r'(?:address|ƒë·ªãa ch·ªâ)[:\s]*([^,\n]+)'
    id_pattern = r'(?:id|number|s·ªë)[:\s]*([A-Z0-9]+)'
    
    names = re.findall(name_pattern, extracted_text, re.IGNORECASE)
    addresses = re.findall(address_pattern, extracted_text, re.IGNORECASE)
    ids = re.findall(id_pattern, extracted_text, re.IGNORECASE)
    
    names = list(set([name.strip() for name in names if len(name.strip()) > 2]))
    addresses = list(set([addr.strip() for addr in addresses if len(addr.strip()) > 5]))
    ids = list(set([id_num.strip() for id_num in ids if len(id_num.strip()) > 3]))
    
    result = f"""üë• MULTIPLE ENTITIES ANALYSIS
    
üìä Detected entities:
"""
    
    if len(names) > 1:
        result += f"üë§ Names ({len(names)}):\n"
        for i, name in enumerate(names, 1):
            result += f"   {i}. {name}\n"
    
    if len(addresses) > 1:
        result += f"üè† Addresses ({len(addresses)}):\n"
        for i, addr in enumerate(addresses, 1):
            result += f"   {i}. {addr}\n"
    
    if len(ids) > 1:
        result += f"üÜî ID Numbers ({len(ids)}):\n"
        for i, id_num in enumerate(ids, 1):
            result += f"   {i}. {id_num}\n"
    
    total_entities = len(names) + len(addresses) + len(ids)
    
    if total_entities > 3:
        result += f"""
‚ö†Ô∏è MULTIPLE ENTITIES DETECTED:
- C√≥ th·ªÉ l√† t√†i li·ªáu gia ƒë√¨nh ho·∫∑c nh√≥m
- C·∫ßn x√°c ƒë·ªãnh th·ª±c th·ªÉ ch√≠nh
- Ki·ªÉm tra m·ªëi quan h·ªá gi·ªØa c√°c th·ª±c th·ªÉ

üí° RECOMMENDATION:
- S·ª≠ d·ª•ng th·ª±c th·ªÉ ƒë·∫ßu ti√™n l√†m ch√≠nh
- L∆∞u tr·ªØ c√°c th·ª±c th·ªÉ kh√°c nh∆∞ th√¥ng tin ph·ª•
"""
    else:
        result += "\n‚úÖ S·ªë l∆∞·ª£ng th·ª±c th·ªÉ trong m·ª©c b√¨nh th∆∞·ªùng"
    
    result += "\n‚úÖ Entity analysis completed!"
    
    return result

def create_classify_agent():
    """
    T·∫°o Classification Agent v·ªõi t·∫•t c·∫£ c√°c tools
    
    Returns:
        Agent: Classification agent ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh
    """
    classify_agent = Agent(
        model=bedrock_model,
        name="document_classifier",
        tools=[classify_document_type, extract_key_fields, detect_multiple_documents, handle_multiple_entities],
        system_prompt="""B·∫°n l√† chuy√™n gia ph√¢n lo·∫°i v√† tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ t√†i li·ªáu.

Nhi·ªám v·ª• c·ªßa b·∫°n:
1. Ph√¢n lo·∫°i t√†i li·ªáu theo 3 category: Identity, Address, Eligibility
2. X√°c ƒë·ªãnh t√™n c·ª• th·ªÉ c·ªßa gi·∫•y t·ªù (Driver's License, Passport, v.v.)
3. Tr√≠ch xu·∫•t c√°c tr∆∞·ªùng d·ªØ li·ªáu quan tr·ªçng
4. X·ª≠ l√Ω edge cases: nhi·ªÅu t√†i li·ªáu, nhi·ªÅu th·ª±c th·ªÉ

Quy tr√¨nh l√†m vi·ªác:
1. S·ª≠ d·ª•ng classify_document_type ƒë·ªÉ ph√¢n lo·∫°i
2. S·ª≠ d·ª•ng extract_key_fields ƒë·ªÉ tr√≠ch xu·∫•t d·ªØ li·ªáu
3. S·ª≠ d·ª•ng detect_multiple_documents n·∫øu nghi ng·ªù c√≥ nhi·ªÅu t√†i li·ªáu
4. S·ª≠ d·ª•ng handle_multiple_entities n·∫øu ph√°t hi·ªán nhi·ªÅu th·ª±c th·ªÉ

Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, chi ti·∫øt v√† ch√≠nh x√°c."""
    )
    return classify_agent

